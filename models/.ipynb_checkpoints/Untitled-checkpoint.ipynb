{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba177c6-550e-4765-8728-4435b0337b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL FOR LightGBM with full dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import lightgbm as lgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import cdist\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Load and explore your dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Sample data:\\n{df.head()}\")\n",
    "\n",
    "    # Make sure the column names match what we need\n",
    "    # If your CSV has different column names, rename them here\n",
    "    if 'word' not in df.columns:\n",
    "        if 'term' in df.columns:\n",
    "            df.rename(columns={'term': 'word'}, inplace=True)\n",
    "        elif 'Word' in df.columns:\n",
    "            df.rename(columns={'Word': 'word'}, inplace=True)\n",
    "        else:\n",
    "            # Try to identify the column that contains words\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object' and df[col].str.isalpha().mean() > 0.8:\n",
    "                    print(f\"Renaming column '{col}' to 'word'\")\n",
    "                    df.rename(columns={col: 'word'}, inplace=True)\n",
    "                    break\n",
    "\n",
    "    # Similarly for video_link column\n",
    "    if 'video_link' not in df.columns:\n",
    "        if 'url' in df.columns:\n",
    "            df.rename(columns={'url': 'video_link'}, inplace=True)\n",
    "        elif 'URL' in df.columns:\n",
    "            df.rename(columns={'URL': 'video_link'}, inplace=True)\n",
    "        elif 'video_url' in df.columns:\n",
    "            df.rename(columns={'video_url': 'video_link'}, inplace=True)\n",
    "        else:\n",
    "            # Try to identify the column that contains URLs\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object' and df[col].str.contains('http').mean() > 0.5:\n",
    "                    print(f\"Renaming column '{col}' to 'video_link'\")\n",
    "                    df.rename(columns={col: 'video_link'}, inplace=True)\n",
    "                    break\n",
    "\n",
    "    # Make sure we have the required columns\n",
    "    if 'word' not in df.columns or 'video_link' not in df.columns:\n",
    "        print(\"Warning: Required columns 'word' and 'video_link' not found. Please check your CSV file.\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "        # As a last resort, create default columns\n",
    "        if 'word' not in df.columns:\n",
    "            print(\"Creating a default 'word' column using the first text column\")\n",
    "            text_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "            if text_cols:\n",
    "                df['word'] = df[text_cols[0]]\n",
    "\n",
    "        if 'video_link' not in df.columns:\n",
    "            print(\"Creating a placeholder 'video_link' column\")\n",
    "            df['video_link'] = [f\"https://example.com/video_{i}\" for i in range(len(df))]\n",
    "\n",
    "    # Clean up the word column - lowercase, strip whitespace\n",
    "    df['word'] = df['word'].str.lower().str.strip()\n",
    "\n",
    "    # Remove duplicates if any\n",
    "    df = df.drop_duplicates(subset=['word'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 2: Generate embeddings using a pre-trained model\n",
    "def generate_embeddings(df):\n",
    "    print(\"Generating word embeddings...\")\n",
    "    # Using Sentence Transformers for word embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for each word\n",
    "    word_embeddings = {}\n",
    "    for word in df['word'].unique():\n",
    "        word_embeddings[word] = model.encode([word])[0]\n",
    "\n",
    "    # Convert embeddings to a DataFrame for easier manipulation\n",
    "    embeddings_list = list(word_embeddings.values())\n",
    "    words_list = list(word_embeddings.keys())\n",
    "\n",
    "    # Create a DataFrame with word and its embedding\n",
    "    embeddings_df = pd.DataFrame({\n",
    "        'word': words_list,\n",
    "        'embedding': embeddings_list\n",
    "    })\n",
    "\n",
    "    # Merge with original DataFrame\n",
    "    df_with_embeddings = df.merge(embeddings_df, on='word')\n",
    "\n",
    "    return df_with_embeddings, word_embeddings, embeddings_list, words_list\n",
    "\n",
    "# Step 3: Automatically categorize words using clustering\n",
    "def auto_categorize_words(df, word_embeddings):\n",
    "    print(\"Automatically categorizing words using clustering...\")\n",
    "    words = list(word_embeddings.keys())\n",
    "    embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "    # Option 1: K-Means clustering\n",
    "    # We'll try to determine the optimal number of clusters using the elbow method\n",
    "    inertias = []\n",
    "    max_clusters = min(15, len(words) - 1)  # Don't try more clusters than we have words, minus 1\n",
    "\n",
    "    if max_clusters > 1:\n",
    "        for k in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(embeddings)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "\n",
    "        # Plot the elbow curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(2, max_clusters + 1), inertias, marker='o')\n",
    "        plt.title('Elbow Method For Optimal Number of Clusters')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.savefig('elbow_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Heuristic: Find the \"elbow\" point - where the rate of decrease sharply changes\n",
    "        # Simple approach: look for the largest second derivative\n",
    "        diffs = np.diff(inertias)\n",
    "        second_diffs = np.diff(diffs)\n",
    "        if len(second_diffs) > 0:\n",
    "            elbow_idx = np.argmax(second_diffs) + 2  # +2 because of the two diffs and 0-indexing\n",
    "            optimal_k = elbow_idx + 2  # +2 because our range started at 2\n",
    "        else:\n",
    "            optimal_k = 3  # Default if we can't compute second derivatives\n",
    "    else:\n",
    "        optimal_k = 1\n",
    "\n",
    "    print(f\"Estimated optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "    # Option 2: HDBSCAN for clustering (often better for text embeddings)\n",
    "    # Reduce dimensionality first for better clustering\n",
    "    umap_reducer = umap.UMAP(n_components=10, random_state=42)\n",
    "    umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Apply HDBSCAN clustering\n",
    "    hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=max(2, len(words) // 20),\n",
    "                                        min_samples=1,\n",
    "                                        prediction_data=True)\n",
    "    hdbscan_labels = hdbscan_clusterer.fit_predict(umap_embeddings)\n",
    "\n",
    "    # Fall back to K-means if HDBSCAN didn't find good clusters\n",
    "    if len(np.unique(hdbscan_labels)) < 2 or (len(np.unique(hdbscan_labels)) == 2 and -1 in hdbscan_labels):\n",
    "        print(\"HDBSCAN didn't find clear clusters, falling back to K-means\")\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    else:\n",
    "        cluster_labels = hdbscan_labels\n",
    "        # Relabel noise points (-1) to their nearest cluster\n",
    "        if -1 in cluster_labels:\n",
    "            noise_indices = np.where(cluster_labels == -1)[0]\n",
    "            for idx in noise_indices:\n",
    "                # Find the nearest cluster center\n",
    "                if len(np.unique(cluster_labels)) > 1:  # Ensure we have clusters\n",
    "                    unique_clusters = [c for c in np.unique(cluster_labels) if c != -1]\n",
    "                    cluster_points = [embeddings[cluster_labels == c].mean(axis=0) for c in unique_clusters]\n",
    "                    distances = cdist([embeddings[idx]], cluster_points, 'euclidean')[0]\n",
    "                    nearest_cluster = unique_clusters[np.argmin(distances)]\n",
    "                    cluster_labels[idx] = nearest_cluster\n",
    "\n",
    "    # Create a mapping from words to clusters\n",
    "    word_to_cluster = {word: label for word, label in zip(words, cluster_labels)}\n",
    "\n",
    "    # Add cluster labels to the dataframe\n",
    "    df['auto_category'] = df['word'].map(word_to_cluster)\n",
    "\n",
    "    # Visualize the clusters in 2D\n",
    "    # Use t-SNE for dimensionality reduction to 2D for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.8)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "    # Annotate some points with word labels (annotate a subset to avoid overcrowding)\n",
    "    max_annotations = min(50, len(words))\n",
    "    step = max(1, len(words) // max_annotations)\n",
    "    for i in range(0, len(words), step):\n",
    "        plt.annotate(words[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8)\n",
    "\n",
    "    plt.title('Word Clusters Visualization')\n",
    "    plt.savefig('word_clusters.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print examples of words in each cluster\n",
    "    print(\"\\nWord categories discovered by clustering:\")\n",
    "    for cluster in sorted(np.unique(cluster_labels)):\n",
    "        cluster_words = [words[i] for i in range(len(words)) if cluster_labels[i] == cluster]\n",
    "        cluster_examples = cluster_words[:min(5, len(cluster_words))]\n",
    "        print(f\"Cluster {cluster}: {', '.join(cluster_examples)}{'...' if len(cluster_words) > 5 else ''}\")\n",
    "\n",
    "    return df, word_to_cluster\n",
    "\n",
    "# Step 4: Calculate similarity scores between words\n",
    "def calculate_similarities(word_embeddings):\n",
    "    print(\"Calculating similarity scores between words...\")\n",
    "    words = list(word_embeddings.keys())\n",
    "    embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Create a DataFrame for the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "\n",
    "    return similarity_df\n",
    "\n",
    "# Step 5: Prepare data for LightGBM model\n",
    "def prepare_training_data(df, similarity_df, word_embeddings):\n",
    "    print(\"Preparing training data for LightGBM model...\")\n",
    "    # Create training data with features and target\n",
    "    training_data = []\n",
    "\n",
    "    words = list(word_embeddings.keys())\n",
    "\n",
    "    for i, word1 in enumerate(words):\n",
    "        category1 = df[df['word'] == word1]['auto_category'].iloc[0]\n",
    "\n",
    "        for j, word2 in enumerate(words):\n",
    "            if word1 != word2:\n",
    "                category2 = df[df['word'] == word2]['auto_category'].iloc[0]\n",
    "                similarity = similarity_df.loc[word1, word2]\n",
    "\n",
    "                # Features: word embedding of word1 and word2\n",
    "                embedding1 = word_embeddings[word1]\n",
    "                embedding2 = word_embeddings[word2]\n",
    "\n",
    "                # Combine features\n",
    "                features = np.concatenate([embedding1, embedding2])\n",
    "\n",
    "                # Target: 1 if same category, 0 otherwise\n",
    "                target = 1 if category1 == category2 else 0\n",
    "\n",
    "                training_data.append({\n",
    "                    'word1': word1,\n",
    "                    'word2': word2,\n",
    "                    'features': features,\n",
    "                    'similarity': similarity,\n",
    "                    'target': target\n",
    "                })\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    train_df = pd.DataFrame(training_data)\n",
    "\n",
    "    # Split features into separate columns for LightGBM\n",
    "    feature_columns = [f'feature_{i}' for i in range(len(train_df['features'].iloc[0]))]\n",
    "    features_df = pd.DataFrame(train_df['features'].tolist(), columns=feature_columns)\n",
    "\n",
    "    # Combine with training DataFrame\n",
    "    train_df = pd.concat([train_df.drop('features', axis=1), features_df], axis=1)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# Step 6: Train LightGBM model\n",
    "def train_lightgbm_model(train_df):\n",
    "    print(\"Training LightGBM model...\")\n",
    "    # Split data into features and target\n",
    "    feature_cols = [col for col in train_df.columns if col.startswith('feature_')]\n",
    "    X = train_df[feature_cols]\n",
    "    y = train_df['target']\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # Train LightGBM model\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[\n",
    "        lgb.log_evaluation(period=100),  # This replaces verbose_eval\n",
    "        # You can add early_stopping here too\n",
    "        lgb.early_stopping(10)]\n",
    "    )\n",
    "\n",
    "    '''from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)  # Fit only on training data\n",
    "    X_test = scaler.transform(X_test)  # Transform test data without fitting'''\n",
    "\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"Train R²:\", r2_score(y_train, y_pred_train))\n",
    "    print(\"Test R²:\", r2_score(y_test, y_pred_test))  # Should be lower than train score\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nModel Evaluation Metrics:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "    return model, X_test, y_test, y_pred\n",
    "\n",
    "# Step 7: Create the recommendation function\n",
    "def recommend_words(input_word, df, similarity_df, model, word_embeddings, top_n=5):\n",
    "    if input_word not in df['word'].unique():\n",
    "        print(f\"Word '{input_word}' not found in the dataset\")\n",
    "        return []\n",
    "\n",
    "    # Get the category of the input word\n",
    "    category = df[df['word'] == input_word]['auto_category'].iloc[0]\n",
    "\n",
    "    # Get embedding of the input word\n",
    "    input_embedding = word_embeddings[input_word]\n",
    "\n",
    "    # Prepare data for model prediction\n",
    "    recommendations = []\n",
    "\n",
    "    for word in df['word'].unique():\n",
    "        if word != input_word:\n",
    "            # Get embedding of the candidate word\n",
    "            candidate_embedding = word_embeddings[word]\n",
    "\n",
    "            # Get category of candidate word\n",
    "            candidate_category = df[df['word'] == word]['auto_category'].iloc[0]\n",
    "\n",
    "            # Calculate similarity score from pre-computed similarity matrix\n",
    "            similarity = similarity_df.loc[input_word, word]\n",
    "\n",
    "            # Add to recommendations with all relevant information\n",
    "            recommendations.append({\n",
    "                'word': word,\n",
    "                'category': candidate_category,\n",
    "                'same_category': category == candidate_category,\n",
    "                'similarity': similarity,\n",
    "                'video_link': df[df['word'] == word]['video_link'].iloc[0]\n",
    "            })\n",
    "\n",
    "    # Sort by similarity score\n",
    "    recommendations = sorted(recommendations, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    # Filter by category\n",
    "    category_recommendations = [r for r in recommendations if r['same_category']]\n",
    "\n",
    "    # If not enough same-category recommendations, add some from other categories\n",
    "    if len(category_recommendations) < top_n:\n",
    "        other_recommendations = [r for r in recommendations if not r['same_category']]\n",
    "        category_recommendations.extend(other_recommendations[:top_n - len(category_recommendations)])\n",
    "\n",
    "    # Return top N recommendations\n",
    "    return category_recommendations[:top_n]\n",
    "\n",
    "# Step 8: Visualize similarity scores\n",
    "def visualize_similarity(similarity_df, df):\n",
    "    print(\"Visualizing word similarities...\")\n",
    "    # Get a subset of words for visualization (to avoid overcrowding)\n",
    "    categories = df['auto_category'].unique()\n",
    "    sample_words = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_words = df[df['auto_category'] == category]['word'].unique()\n",
    "        if len(category_words) > 0:\n",
    "            sample_words.extend(category_words[:min(3, len(category_words))])\n",
    "\n",
    "    # Limit to a reasonable number for visualization\n",
    "    max_words = min(20, len(sample_words))\n",
    "    sample_words = sample_words[:max_words]\n",
    "\n",
    "    # Create a subset of the similarity matrix\n",
    "    subset_similarity = similarity_df.loc[sample_words, sample_words]\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(subset_similarity, annot=True, cmap='viridis', fmt='.2f')\n",
    "    plt.title('Word Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_similarity_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Function to evaluate the recommendation system\n",
    "def evaluate_recommendations(recommend_function, df, word_embeddings, n_samples=10):\n",
    "    print(\"\\nEvaluating recommendation system...\")\n",
    "    # Sample a few words to test\n",
    "    test_words = np.random.choice(list(df['word'].unique()), size=min(n_samples, len(df['word'].unique())), replace=False)\n",
    "\n",
    "    results = []\n",
    "    for word in test_words:\n",
    "        recommendations = recommend_function(word)\n",
    "        avg_similarity = np.mean([rec['similarity'] for rec in recommendations]) if recommendations else 0\n",
    "        same_category_count = sum(1 for rec in recommendations if rec['same_category']) if recommendations else 0\n",
    "\n",
    "        results.append({\n",
    "            'word': word,\n",
    "            'category': df[df['word'] == word]['auto_category'].iloc[0],\n",
    "            'avg_similarity': avg_similarity,\n",
    "            'same_category_ratio': same_category_count / len(recommendations) if recommendations else 0,\n",
    "            'num_recommendations': len(recommendations)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(f\"Average similarity score across all recommendations: {results_df['avg_similarity'].mean():.4f}\")\n",
    "    print(f\"Average same-category ratio: {results_df['same_category_ratio'].mean():.4f}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main(file_path):\n",
    "    print(\"Starting word recommendation system with your data...\")\n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # Generate embeddings\n",
    "    df_with_embeddings, word_embeddings, embeddings_list, words_list = generate_embeddings(df)\n",
    "\n",
    "    # Automatically categorize words\n",
    "    df_categorized, word_to_cluster = auto_categorize_words(df_with_embeddings, word_embeddings)\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarity_df = calculate_similarities(word_embeddings)\n",
    "\n",
    "    # Visualize similarity\n",
    "    visualize_similarity(similarity_df, df_categorized)\n",
    "\n",
    "    # Prepare training data\n",
    "    train_df = prepare_training_data(df_categorized, similarity_df, word_embeddings)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model, X_test, y_test, y_pred = train_lightgbm_model(train_df)\n",
    "\n",
    "\n",
    "    ##########\n",
    "\n",
    "    # Create recommendation function\n",
    "    def get_recommendations(word, top_n=5):\n",
    "        return recommend_words(word, df_categorized, similarity_df, model, word_embeddings, top_n)\n",
    "\n",
    "    # Test with a few examples\n",
    "    print(\"\\nTesting recommendation system with examples:\")\n",
    "    for example_word in np.random.choice(df['word'].unique(), size=min(3, len(df['word'].unique())), replace=False):\n",
    "        recommendations = get_recommendations(example_word)\n",
    "\n",
    "        print(f\"\\nRecommendations for '{example_word}':\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec['word']} (Similarity: {rec['similarity']:.4f}, Same Category: {'Yes' if rec['same_category'] else 'No'})\")\n",
    "            print(f\"   Video Link: {rec['video_link']}\")\n",
    "\n",
    "    # Evaluate recommendations\n",
    "    eval_results = evaluate_recommendations(get_recommendations, df_categorized, word_embeddings)\n",
    "\n",
    "    # Save the trained model and necessary data for later use\n",
    "    model_data = {\n",
    "        'word_embeddings': word_embeddings,\n",
    "        'similarity_df': similarity_df,\n",
    "        'categorized_df': df_categorized\n",
    "    }\n",
    "\n",
    "    # Return the recommendation function for interactive use\n",
    "    return get_recommendations, model_data\n",
    "\n",
    "# Interactive component for user input\n",
    "def interactive_recommendations(recommend_function):\n",
    "    while True:\n",
    "        word = input(\"\\nEnter a word for recommendations (or 'q' to quit): \")\n",
    "        if word.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        recommendations = recommend_function(word)\n",
    "\n",
    "        if not recommendations:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nRecommendations for '{word}':\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec['word']} (Similarity: {rec['similarity']:.4f}, Same Category: {'Yes' if rec['same_category'] else 'No'})\")\n",
    "            print(f\"   Video Link: {rec['video_link']}\")\n",
    "\n",
    "# Run the system with your data\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Desktop/models/Final_Expanded_1000_Words_dataset.csv\"  # Your actual file path\n",
    "    recommend_function, model_data = main(file_path)\n",
    "\n",
    "    # Option to run interactive mode\n",
    "    interactive_recommendations(recommend_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
